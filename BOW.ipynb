{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYTcCcW1rFseA/sexFuzyj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Vaidehi Kale - 25**"],"metadata":{"id":"6whpjYKViJ2z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4MUFgxe5P9p"},"outputs":[],"source":["#Libraries\n","from tensorflow import keras\n","from typing import List\n","from keras.preprocessing.text import Tokenizer"]},{"cell_type":"code","source":["sentence= [\"John likes to watch movies. Mary likes movies too.\"]\n","def print_bow(sentence:List[str])->None:\n","  tokenizer=Tokenizer()   #create object of Tokenizer()\n","  tokenizer.fit_on_texts(sentence)\n","  sequences= tokenizer.texts_to_sequences(sentence)\n","  word_index = tokenizer.word_index\n","  bow= {}\n","  for key in word_index:\n","    bow[key]=sequences[0].count(word_index[key])\n","  #f-string : convenient way to embed Python expressions inside string literals for formatting.\n","  print(f\"Bag of word sentence 1: {bow}\")\n","  print(f\"We found {len(word_index)} unique tokens.\")\n","  #  print(\"We found\", len(word_index),\"unique tokens.\")\n","print_bow(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_d9ALGIJ6fGK","executionInfo":{"status":"ok","timestamp":1710756696933,"user_tz":-330,"elapsed":577,"user":{"displayName":"VAIDEHI KALE","userId":"01679255543626581497"}},"outputId":"68cb9ef9-b6ee-4292-81a1-49fc514ec84b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bag of word sentence 1: {'likes': 2, 'movies': 2, 'john': 1, 'to': 1, 'watch': 1, 'mary': 1, 'too': 1}\n","We found 7 unique tokens.\n"]}]},{"cell_type":"code","source":["s1 = [\"Rucha likes to eat cakes ,whereas Riya Likes ice-cream.\"]\n","def print_bow(s1:List[str])->None:\n","  tokenizer=Tokenizer()   #create object of Tokenizer()\n","  tokenizer.fit_on_texts(s1)\n","  sequences= tokenizer.texts_to_sequences(s1)\n","  word_index = tokenizer.word_index\n","  bow= {}\n","  for key in word_index:\n","    bow[key]=sequences[0].count(word_index[key])\n","  #f-string : convenient way to embed Python expressions inside string literals for formatting.\n","  print(f\"Bag of word sentence 1: {bow}\")\n","  print(f\"We found {len(word_index)} unique tokens.\")\n","  #  print(\"We found\", len(word_index),\"unique tokens.\")\n","\n","print_bow(s1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8dvIQmhCCnk","executionInfo":{"status":"ok","timestamp":1710756699332,"user_tz":-330,"elapsed":4,"user":{"displayName":"VAIDEHI KALE","userId":"01679255543626581497"}},"outputId":"1ac8bf28-25b3-4fbe-dff3-686603f5c9fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bag of word sentence 1: {'likes': 2, 'rucha': 1, 'to': 1, 'eat': 1, 'cakes': 1, 'whereas': 1, 'riya': 1, 'ice': 1, 'cream': 1}\n","We found 9 unique tokens.\n"]}]}]}